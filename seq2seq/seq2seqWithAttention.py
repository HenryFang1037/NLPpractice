# -*- coding:utf-8 -*-
###########################################################################################
# æœ¬æ–‡åŸºæœ¬æ¨¡ä»¿è‡ªhttps://github.com/NELSONZHAO/zhihuä¸­çš„Seq2Seq Attention Keras å®ç°æ–¹æ³•       #
# ç»è¿‡ä¸€æ®µæ—¶é—´çš„å´æ©è¾¾ã€æå®æ¯…è€å¸ˆçš„è§†é¢‘è¯¾ç¨‹å­¦ä¹ ï¼Œå†³å®šåŠ¨æ‰‹å®ç°ä¸€ä¸‹ä»£ç ï¼Œä½†æ˜¯å‘ç°ä¸€åˆ°åŠ¨æ‰‹å†™ä»£ç çš„        #
# æ—¶å€™å°±è§‰çš„æ— ä»ä¸‹æ‰‹ï¼Œè¿™å¯èƒ½ä¹Ÿæ˜¯åªçœ‹ç†è®ºä¸åŠ¨æ‰‹æ•²ä»£ç å®è·µçš„ç»“æœï¼Œå› æ­¤ä»ç½‘ä¸Šæ‰¾å…¬å¼€çš„èµ„æ–™ä»å¤´å®ç°ä¸€        #
# éã€‚ æœ¬ä»£ç å®ç°çš„è‹±æ³•ç¿»è¯‘ä¸­çš„Attentionä¸­Encoderä¸­çš„HiddenStateå€¼ä¸Decoderä¸­çš„HiddenSate       #
# å€¼çš„ç›¸ä¼¼åº¦è®¡ç®—é‡‡ç”¨äº†ï¼ˆ1ï¼šç‚¹ç§¯ Dot; 2:cosineç›¸ä¼¼åº¦ï¼›3ï¼šMLPç½‘ç»œï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰ï¼‰ä¸­çš„1æ–¹æ³•ã€‚          #
# å…³äºAttentionä¸­ç›¸ä¼¼åº¦è®¡ç®—çš„ä¸‰ç§æ–¹å¼ï¼Œå¯ç§»æ­¥https://www.cnblogs.com/guoyaohua/p/9429924.html  #
# å†æ¬¡æ„Ÿè°¢ğŸ™åšä¸»çš„ä»£ç , åšä¸»çŸ¥ä¹ä¸Šçš„ç²¾å½©åˆ†äº«åœ°å€ï¼šhttps://zhuanlan.zhihu.com/p/37290775         #
###########################################################################################

import os, re, jieba, tqdm
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import keras.backend as K
from datetime import datetime
from keras.optimizers import adam
from keras.utils import to_categorical
from keras.models import Model
from keras.layers import Concatenate, Reshape, Dot, RepeatVector, Activation
from keras.layers import Input, Dense, LSTM, Bidirectional, Embedding
from nltk.translate.bleu_score import sentence_bleu

# è®­ç»ƒèµ„æ–™å¯åˆ°æœ¬æ–‡å¼€å¤´çš„Githubåœ°å€å»clone
englishText_path = 'zhihu/mt_attention_birnn/data/small_vocab_en'
frenchText_path ='zhihu/mt_attention_birnn/data/small_vocab_fr'
# gloveé‡‡ç”¨å“ˆä½›å¤§å­¦100ç»´çš„è¯å‘é‡, ä¸‹è½½åœ°å€å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°é•œåƒå»ä¸‹è½½ï¼ŒåŸç½‘å€ä¸‹è½½é€Ÿåº¦å¤ªæ…¢ï¼
glove_path = '/downloads/glove.6B/glove.6B.100d.txt'


def text2dict(path, source=True):
    """
    :param path: æ–‡æ¡£åœ°å€
    :param source: æ˜¯å¦ä¸ºæºæ–‡æ¡£ï¼Œboolå€¼ï¼ˆTrue/Falseï¼‰
    :return:
    """
    # '<PAD>'ä»£è¡¨è¡¥ä½æ ‡è®°, '<UNK>'ä»£è¡¨æœªå½•å…¥è¯å…¸çš„æœªçŸ¥è¯æ±‡
    # '<BOS>'ä»£è¡¨å¥å­å¼€å¤´, '<EOS>'ä»£è¡¨å¥å­æœ«å°¾
    sourcepattern = ['<PAD>', '<UNK>']
    targetpattern = ['<PAD>', '<UNK>', '<BOS>', 'EOS']
    with open(path, 'r', encoding='utf-8') as f:
        text = f.read()
    sentences = text.split('\n')
    sentencescount = np.sum(sentences)
    wordscount = [len(sentence.split(' ')) for sentence in sentences]
    totalwordscount = np.sum(wordscount)
    averagewordsnumpersent = np.average(wordscount)
    maxwordsnumpersent = np.max(wordscount)
    vocabulary = list(set(text.lower().split()))
    if source is True:
        word2id = {word: idx for idx, word in enumerate(sourcepattern + vocabulary)}
        id2word = {idx: word for word, idx in word2id.items()}
    else:
        word2id = {word: idx for idx, word in enumerate(targetpattern + vocabulary)}
        id2word = {idx: word for word, idx in word2id.items()}
    dictsize = len(word2id)
    print('---' * 15)
    print('The number of sentence in this text is {}'.format(sentencescount))
    print('The number of words in this text is {}'. format(totalwordscount))
    print('The average number of words in a sentence is {}'.format(averagewordsnumpersent))
    print('The max number of words in a sentence is {}'.format(maxwordsnumpersent))
    print('The dict size of this text is {}'.format(dictsize))
    print('---' * 15)
    return sentences, word2id, id2word, dictsize, maxwordsnumpersent


def sentence2id(sentence, word2id, maxlen=20, source=True):
    """
    :param sentence: å°†å¥å­å­—ç¬¦ç¼–ç ä¸ºæ•°å­—æ ‡è¯†ç¬¦
    :param word2id: keyä¸ºword, valueä¸ºidçš„å­—å…¸
    :param maxlen: å¥å­çš„æœ€å¤§é•¿åº¦ï¼ˆå½“å¥å­é•¿åº¦å¤§äºmaxlenæ—¶ï¼Œ
                   æˆªå–maxlenä¹‹å‰çš„æ ‡è¯†ç¬¦ï¼Œå½“å¥å­é•¿åº¦å°äºmaxlenæ—¶è¡¥'<PAD>'ï¼‰
    :param source: æ˜¯å¦ä¸ºæºæ–‡æ¡£ï¼Œboolå€¼ï¼ˆTrue/Falseï¼‰
    :return:
    """
    idx = []
    unk_id = word2id.get('<UNK>')
    pad_id = word2id.get('<PAD>')
    eos_id = word2id.get('<EOS>')
    if source is True:
        for word in sentence.lower().split():
            idx.append(word2id.get(word, unk_id))
    else:
        for word in sentence.lower().split():
            idx.append(word2id.get(word, unk_id))
        idx.append(eos_id)
    if len(idx) > maxlen:
        idx = idx[:maxlen]
    else:
        idx = idx + [pad_id] * (maxlen - len(idx))
    return idx


def text2id(path, source=True):
    """
    :param path: æ–‡æ¡£è·¯å¾„
    :param source: æ˜¯å¦ä¸ºæºæ–‡æ¡£ï¼Œboolå€¼ï¼ˆTrue/Falseï¼‰
    :return:
    """
    sentences, word2id, id2word, dictsize, maxlen = text2dict(path, source=source)
    textidx = []
    for sentence in tqdm.tqdm(sentences):
        idx = sentence2id(sentence, word2id, maxlen=maxlen, source=source)
        textidx.append(idx)
    textidx = np.array(textidx, dtype=np.float32)
    print('The text words have been translated into unique id successfully !')
    return textidx, word2id, id2word, dictsize, maxlen


def getGlove(path):
    """
    :param path: gloveæ–‡æ¡£å­˜å‚¨åœ°å€
    :return: keyä¸ºword, valueä¸ºè¯æ‰€å¯¹åº”çš„è¯å‘é‡
    """
    with open(path, 'r', encoding='utf-8') as f:
        word2vect = {}
        for line in f:
            line = line.strip().split()
            word = line[0]
            word2vect[word] = np.array(line[1:], dtype=np.float32)
    print('The glove text has been converted into word2vector dict !')
    return word2vect


def pretrainedembedding(path, word2id, maxlen):
    """
    :param path: gloveæ–‡æ¡£å­˜å‚¨åœ°å€
    :param word2id: keyä¸ºword, valueä¸ºidçš„å­—å…¸
    :param maxlen: è¾“å…¥å¥å­çš„æœ€å¤§é•¿åº¦
    :return: keras embeddinglayer
    """
    word2vect = getGlove(path)
    # Keras APIè¦æ±‚çš„ Embedding size è¦æ¯”å­—å…¸å¤§ä¸€ç»´
    vocabulary_size = len(word2id) + 1
    # è¯å‘é‡ç»´åº¦ï¼ˆæœ¬æ–‡ä¸­ä½¿ç”¨çš„gloveç»´åº¦ä¸º100ï¼‰
    embedding_dim = word2vect['the'].shape[0]
    embedding_matrix = np.zeros(shape=(vocabulary_size, embedding_dim), dtype=np.float32)
    for word, idx in word2id.items():
        vector = word2vect.get(word, np.zeros(embedding_dim))
        embedding_matrix[idx, :] = vector
    embeddinglayer = Embedding(vocabulary_size, embedding_dim,
                               weights=[embedding_matrix],
                               input_length=maxlen,
                               trainable=False, name='PreTrainedEmbedding')
    print('Pre-trained embedding layer loaded completely !')
    return embeddinglayer


# æœ€å¥½ç»™kerasæ¯ä¸ªlayerèµ·åå­—ï¼Œåç»­æŸ¥çœ‹æ¯ä¸ªlayerè¾“å‡ºæ—¶æ¯”è¾ƒæ–¹ä¾¿
def attention(h, s, sourceSentenceLen):
    """
    :param h: encoderéšå±‚çŠ¶æ€å€¼ï¼ˆhä¸ºtæ—¶åˆ»çš„å€¼ï¼‰
    :param s: decoderéšå±‚çŠ¶æ€å€¼ï¼ˆsä¸ºt-1æ—¶åˆ»çš„å€¼ï¼‰
    :param sourceSentenceLen: encoderè¾“å…¥é•¿åº¦
    :return: cotextå€¼
    """
    s = RepeatVector(sourceSentenceLen, name='Repeat_s')(s)
    concat = Concatenate(axis=-1, name='Concateh&s')([h, s])
    densetanh = Dense(32, activation='tanh', name='DenseTanh32')(concat)
    denserelu = Dense(1, activation='relu', name='DenseRelu1')(densetanh)
    alphas = Activation(activation='softmax', name='attentionWeight')(denserelu)
    context = Dot(axes=1, name='ContextByDot')([alphas, h])
    return context


def encoderBiLSTM(units):
    """
    :param units: encoder LSTM unitä¸ªæ•°ï¼Œ BidirectionalLTSMçš„unitä¸ªæ•°ä¸º2å€çš„LSTM unit
    :return: encoder
    """
    biLSTM = Bidirectional(LSTM(units, return_sequences=True))
    return biLSTM


def decoderLSTM(units):
    """
    :param units: decoder LSTM unitä¸ªæ•°
    :return: decoder
    """
    decoder = LSTM(units, return_state=True)
    return decoder


def seq2seqwithAttention(sourceSentenceLen, targetSentenceLen,
                         encoderUnitsnum, decoderUnitsnum,
                         sourceVocabsize, targetVocabsize,
                         embeddingLayer):
    """
    :param sourceSentenceLen: æºè¯­å¥æœ€å¤§é•¿åº¦
    :param targetSentenceLen: ç›®æ ‡è¯­å¥æœ€å¤§é•¿åº¦
    :param encoderUnitsnum: encoderä¸­LSTMæ•°é‡
    :param decoderUnitsnum: decoderä¸­LSTMæ•°é‡
    :param sourceVocabsize: æºè¯­è¨€å­—å…¸å¤§å°ï¼ˆåŒ…æ‹¬'<PAD>', '<UNK>'ï¼‰
    :param targetVocabsize: ç›®æ ‡è¯­è¨€å­—å…¸å¤§å°ï¼ˆåŒ…æ‹¬'<PAD>', '<UNK>', '<BOS>', '<EOS>'ï¼‰
    :param embeddingLayer: é¢„å…ˆè®­ç»ƒå¥½çš„Embeddingå±‚
    :return: model
    """
    x = Input(shape=(sourceSentenceLen, ), name='EncoderInput')
    x = embeddingLayer(x)
    s = Input(shape=(decoderUnitsnum, ), name='DecoderInitialSInput')
    c = Input(shape=(decoderUnitsnum, ), name='DecoderInitialCInput')
    out = Input(shape=(targetSentenceLen, ), name='DecoderInitialOutput')
    out = Reshape((1, targetSentenceLen), name='DecoderOutputReshaper')(out)

    encoder = encoderBiLSTM(encoderUnitsnum)
    decoder = decoderLSTM(decoderUnitsnum)
    outputer = Dense(targetVocabsize, activation='softmax', name='DecoderOutput')

    outputs = []
    h = encoder(x)
    for i in range(targetSentenceLen):
        context = attention(h, s, sourceSentenceLen)
        decoderInput = Concatenate(axis=-1)([context, Reshape((1, targetSentenceLen))[out]])
        s, _, c = decoder(decoderInput, initial_state=[s, c])
        out = outputer(s)
        outputs.append((out))

    model = Model(input=[x, s, c, out], output=[outputs], name='seq2seqWithAttention')
    model.compile(optimizer=adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),
                  metrics=['accuracy'],
                  loss='categorical_crossentropy')
    return model


def prediction(sourceSentence, sourceword2id, sourceSentenceLen, decoderUnitsnum, targetSentenceLen, model):
    """
    :param sourceSentence: è¦ç¿»è¯‘çš„æºå¥å­
    :param sourceword2id: æºè¯­è¨€å­—å…¸
    :param sourceSentenceLen: æºå¥å­é•¿åº¦ï¼ˆpaddingåçš„é•¿åº¦ï¼‰
    :param decoderUnitsnum: decoderä¸­LSTMæ•°é‡
    :param targetSentenceLen: ç›®æ ‡è¯­å¥æœ€å¤§é•¿åº¦
    :param model: è®­ç»ƒå¥½çš„æ¨¡å‹
    :return: ç¿»è¯‘å¥½çš„ç›®æ ‡è¯­å¥
    """

    s = np.zeros((1, decoderUnitsnum))
    c = np.zeros((1, decoderUnitsnum))
    out = np.zeros((1, targetSentenceLen))
    idx = sentence2id(sentence=sourceSentence, word2id=sourceword2id, maxlen=sourceSentenceLen)
    idx = np.array(idx)

    pred = model.predict([idx.reshape(-1, 20), s, c, out])
    predictions = np.argmax(pred, axis=-1)
    idx = [sourceword2id.get(idx[0], '<UNK>') for idx in predictions]
    return ' '.join(idx)


def plotAttention(sentence, sourceSentenceLen, targetSentenceLen, sourceword2id, decoderUnitsnum, model):
    """
    :param sentence: è¦ç¿»è¯‘çš„æºå¥å­
    :param sourceSentenceLen: æºå¥å­é•¿åº¦ï¼ˆpaddingåçš„é•¿åº¦ï¼‰
    :param targetSentenceLen: paddingåçš„é•¿åº¦
    :param sourceword2id: æºè¯­è¨€å­—å…¸
    :param decoderUnitsnum: decoderä¸­LSTMæ•°é‡
    :param model: decoderä¸­LSTMæ•°é‡
    :return:
    """
    x = np.array(sentence2id(sentence, word2id=sourceword2id, maxlen=sourceSentenceLen, source=False))
    f = K.function(model.inputs,
                   [model.get_layer(name='attentionWeight').get_output_at(t) for t in range(targetSentenceLen)])
    s = np.zeros((1, decoderUnitsnum))
    c = np.zeros((1, decoderUnitsnum))
    out = np.zeros((1, targetSentenceLen))

    attention_alpha = f([x.reshape(-1, sourceSentenceLen), s, c, out])
    attention_matrix = np.zeros((targetSentenceLen, sourceSentenceLen))
    for j in range(targetSentenceLen):
        for i in range(sourceSentenceLen):
            attention_matrix[j][i] = attention_alpha[j][0, i, 0]
    y = prediction(sentence, sourceword2id=sourceword2id, sourceSentenceLen=sourceSentenceLen,
                   decoderUnitsnum=decoderUnitsnum, targetSentenceLen=targetSentenceLen, model=model)
    print('The translated sentence is {}'.format(y))
    sourceWordsList = sentence.split()
    targetWordsList = y.split()
    f, axes = plt.subplots(figsize=(15, 10))
    sns.heatmap(attention_matrix, xticklabels=sourceWordsList, yticklabels=targetWordsList, cmap='YlGnBu')
    axes.set_title('Attention Map\n(Target words vs Source words)', fontsize=14)
    plt.show()


if __name__ == '__main__':
    # è®­ç»ƒèµ„æ–™å¯åˆ°æœ¬æ–‡å¼€å¤´çš„Githubåœ°å€å»clone
    englishText_path = 'zhihu/mt_attention_birnn/data/small_vocab_en'
    frenchText_path = 'zhihu/mt_attention_birnn/data/small_vocab_fr'
    # gloveé‡‡ç”¨å“ˆä½›å¤§å­¦100ç»´çš„è¯å‘é‡, ä¸‹è½½åœ°å€å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°é•œåƒå»ä¸‹è½½ï¼ŒåŸç½‘å€ä¸‹è½½é€Ÿåº¦å¤ªæ…¢ï¼
    glove_path = '/downloads/glove.6B/glove.6B.100d.txt'
    # encoder units
    encoderUnitsnum = 32
    # decoder units
    decoderUnitsnum = 128

    sourcetextidx, sourceword2id, sourceid2word, sourcedictsize, sourcesentencemaxlen = text2id(englishText_path,
                                                                                                source=True)
    targettextidx, targetword2id, targetid2word, targetdictsize, targetsentencemaxlen = text2id(frenchText_path,
                                                                                                source=False)
    embeddinglayer = pretrainedembedding(glove_path, sourceword2id, sourcesentencemaxlen)

    s = np.zeros((1, decoderUnitsnum))
    c = np.zeros((1, decoderUnitsnum))
    out = np.zeros((1, targetsentencemaxlen))

    x = sourcetextidx
    y = targettextidx
    # å°†yè½¬æ¢ä¸ºone-hot encoding, æ¨¡å‹æœ€åè®¡ç®—categorical_crossentropyæ—¶è¦ç”¨åˆ°
    y = to_categorical(y, targetdictsize)
    y = list(y.swapaxes(0, 1))

    # æ¨¡å‹è®­ç»ƒ
    model = seq2seqwithAttention(sourceSentenceLen=sourcesentencemaxlen, targetSentenceLen=targetsentencemaxlen,
                                 encoderUnitsnum=encoderUnitsnum, decoderUnitsnum=decoderUnitsnum,
                                 sourceVocabsize=sourcedictsize, targetVocabsize=targetdictsize,
                                 embeddingLayer=embeddinglayer
                                 )
    model.fit(x=[x, s, c, out], y=y, epochs=5, batch_size=128)
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    model.save('seq2seqWithAttention_{}.h5'.format(datetime.now().strftime('%Y-%M-%D')))

    test_sentence = 'she likes mangoes, apples and bananas.'
    plotAttention(test_sentence, sourceSentenceLen=sourcesentencemaxlen,
                  targetSentenceLen=targetsentencemaxlen, sourceword2id=sourceword2id,
                  decoderUnitsnum=decoderUnitsnum, model=model)


    # nltk å¥å­ç¿»è¯‘å¾—åˆ†è®¡ç®— sentence_bleu
    # å¦‚æœæƒ³è¦çœ‹çœ‹è®­ç»ƒçš„æ¨¡å‹æ•ˆæœå¦‚ä½•å¯ä»¥è®¡ç®—bleuå¾—åˆ†
    sentence_bleu([["i", "like", "you", "."]], ["i", "love", "you", "."])






